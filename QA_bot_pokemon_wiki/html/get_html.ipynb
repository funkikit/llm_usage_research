{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.exceptions import ConnectionError\n",
    "import hashlib\n",
    "\n",
    "\n",
    "def get_filename_from_url(url):\n",
    "    # URLからMD5ハッシュを計算\n",
    "    hash_object = hashlib.md5(url.encode())\n",
    "    # ハッシュ値を16進数の文字列に変換\n",
    "    hex_dig = hash_object.hexdigest()\n",
    "    return hex_dig\n",
    "\n",
    "\n",
    "def download_html(url, save_dir):\n",
    "    response = requests.get(url)\n",
    "    filename = os.path.join(save_dir, get_filename_from_url(url.replace('/', '_')) + '.html')\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(response.text)\n",
    "\n",
    "\n",
    "def get_links(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = [a['href'] for a in soup.find_all('a', href=True) if a['href'].startswith('http')]\n",
    "    return links\n",
    "\n",
    "# 初期のハイパーリンク\n",
    "initial_urls = [\n",
    "                'https://wiki.xn--rckteqa2e.com/wiki/%E3%83%9D%E3%82%B1%E3%83%A2%E3%83%B3%E4%B8%80%E8%A6%A7',\n",
    "                ]\n",
    "\n",
    "# HTMLファイルを保存するディレクトリ\n",
    "save_dir = 'downloaded_html_files'\n",
    "os.makedirs(save_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin\n",
    "\n",
    "def get_links(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    links = [urljoin(url, link.get('href')) for link in soup.find_all('a')]\n",
    "    # \"https://ja.wikipedia.org/\"で始まるリンクのみを含める\n",
    "    links = [link for link in links if link.startswith(\"https://wiki.xn--rckteqa2e.com/wiki\")]\n",
    "    return links\n",
    "\n",
    "for initial_url in initial_urls:\n",
    "    try:\n",
    "        # 初期のハイパーリンクのHTMLをダウンロード\n",
    "        download_html(initial_url, save_dir)\n",
    "    except ConnectionError:\n",
    "        print(f\"Failed to connect to {initial_url}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # 初期のハイパーリンクに紐付いたハイパーリンクを取得\n",
    "        linked_urls = get_links(initial_url)\n",
    "    except ConnectionError:\n",
    "        print(f\"Failed to get links from {initial_url}. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    linked_urls = list(set(linked_urls))\n",
    "    \n",
    "    # 紐付いたハイパーリンクのHTMLをダウンロード\n",
    "    for url in linked_urls:\n",
    "        try:\n",
    "            download_html(url, save_dir)\n",
    "        except ConnectionError:\n",
    "            print(f\"Failed to download HTML from {url}. Skipping...\")\n",
    "            continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
